{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Lens data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+\n",
      "|movieId|Title                      |\n",
      "+-------+---------------------------+\n",
      "|1      |Toy Story                  |\n",
      "|2      |Jumanji                    |\n",
      "|3      |Grumpier Old Men           |\n",
      "|4      |Waiting to Exhale          |\n",
      "|5      |Father of the Bride Part II|\n",
      "+-------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "62423 movies\n"
     ]
    }
   ],
   "source": [
    "stripYear = F.udf(lambda title: title[:-7])\n",
    "movies_ddf = (spark.read.csv('movies.csv', header=True, inferSchema=True)\n",
    "              .drop('genres')\n",
    "              .withColumn('Title', stripYear(F.col('title'))))\n",
    "movies_ddf.show(5, False)\n",
    "print(f\"{movies_ddf.count()} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|1     |296    |5.0   |\n",
      "|1     |306    |3.5   |\n",
      "|1     |307    |5.0   |\n",
      "|1     |665    |5.0   |\n",
      "|1     |899    |3.5   |\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "25000095 ratings\n"
     ]
    }
   ],
   "source": [
    "ratings_ddf = (spark.read.csv('ratings.csv', header=True, inferSchema=True)\n",
    "                .drop('timestamp'))\n",
    "ratings_ddf.show(5, False)\n",
    "print(f\"{ratings_ddf.count()} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------------+\n",
      "|movieId|Rating            |Title                    |\n",
      "+-------+------------------+-------------------------+\n",
      "|1088   |3.25002094679514  |Dirty Dancing            |\n",
      "|1580   |3.5817083457378187|Men in Black (a.k.a. MIB)|\n",
      "|3175   |3.6077836141619484|Galaxy Quest             |\n",
      "|44022  |3.2593627146699773|Ice Age 2: The Meltdown  |\n",
      "|175197 |2.754918032786885 |The Dark Tower           |\n",
      "+-------+------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens_ddf = (ratings_ddf\n",
    "  .groupby('movieId')\n",
    "  .avg('rating')\n",
    "  .select(F.col('movieId'), F.col('avg(rating)').alias('Rating'))\n",
    "  .join(movies_ddf, 'movieId'))\n",
    "lens_ddf.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+------+\n",
      "|Id |Title                   |Rating|\n",
      "+---+------------------------+------+\n",
      "|1  |The Shawshank Redemption|9.2   |\n",
      "|2  |The Godfather           |9.2   |\n",
      "|3  |The Godfather: Part II  |9     |\n",
      "|4  |Pulp Fiction            |8.9   |\n",
      "|5  |Schindler's List        |8.9   |\n",
      "+---+------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_df = (spark.read.csv('imdb_sample.csv', sep=';', header='true')\n",
    "           .select('Id', 'Title', F.col('ImdbScore').alias('Rating')))\n",
    "IMDB_df.show(5, False)\n",
    "IMDB_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When joining on (exact) Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = IMDB_df.join(lens_ddf, 'Title')\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage (Fuzzy Matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare join column by doing multiple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example transformation (58818 movies left):\n",
      "+-------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|movieId|        Title|       concat|                char|               ngram|              vector|                 lsh|\n",
      "+-------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   1088|Dirty Dancing|dirty dancing|[d, i, r, t, y,  ...|[d i, i r, r t, t...|(262144,[1726,265...|[[9307048.0], [1....|\n",
      "+-------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, NGram, HashingTF, MinHashLSH, RegexTokenizer, SQLTransformer\n",
    "\n",
    "model = Pipeline(stages=[\n",
    "    SQLTransformer(statement=\"SELECT *, lower(Title) lower FROM __THIS__\"),\n",
    "    Tokenizer(inputCol=\"lower\", outputCol=\"token\"),\n",
    "    StopWordsRemover(inputCol=\"token\", outputCol=\"stop\"),\n",
    "    SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\"),\n",
    "    RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1),\n",
    "    NGram(n=2, inputCol=\"char\", outputCol=\"ngram\"),\n",
    "    HashingTF(inputCol=\"ngram\", outputCol=\"vector\"),\n",
    "    MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "]).fit(lens_ddf)\n",
    "\n",
    "result_lens = model.transform(lens_ddf)\n",
    "result_lens = result_lens.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "print(f\"Example transformation ({result_lens.count()} movies left):\")\n",
    "result_lens.select('movieId', 'Title', 'concat', 'char', 'ngram', 'vector', 'lsh').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out rows: 1\n",
      "+---+-----+------+----+-----+--------------+\n",
      "| id|Title|concat|char|ngram|        vector|\n",
      "+---+-----+------+----+-----+--------------+\n",
      "| 69|    M|     m| [m]|   []|(262144,[],[])|\n",
      "+---+-----+------+----+-----+--------------+\n",
      "\n",
      "Example transformation (99 movies left):\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|               Title|              concat|                char|               ngram|              vector|                 lsh|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|The Shawshank Red...|shawshank redemption|[s, h, a, w, s, h...|[s h, h a, a w, w...|(262144,[5535,212...|[[4.6614889E7], [...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use pipeline previous defined\n",
    "result_imdb = model.transform(IMDB_df)\n",
    "filtered = result_imdb.filter(F.size(F.col(\"ngram\")) < 1)\n",
    "print(f\"Filtered out rows: {filtered.count()}\")\n",
    "filtered.select('id', 'Title', 'concat', 'char', 'ngram', 'vector').show()\n",
    "result_imdb = result_imdb.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "print(f\"Example transformation ({result_imdb.count()} movies left):\")\n",
    "result_imdb.select('id', 'Title', 'concat', 'char', 'ngram', 'vector', 'lsh').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join based on Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335 matches\n",
      "+---+--------------------+--------------------+-------------------+\n",
      "| id|               Title|               Title|        jaccardDist|\n",
      "+---+--------------------+--------------------+-------------------+\n",
      "|  1|The Shawshank Red...|Shawshank Redempt...|0.05555555555555558|\n",
      "|  1|The Shawshank Red...|          Redemption|0.47058823529411764|\n",
      "| 10|          Fight Club|   Female Fight Club|             0.4375|\n",
      "| 10|          Fight Club|          Fight Club|                0.0|\n",
      "| 10|          Fight Club|   Zombie Fight Club|             0.4375|\n",
      "+---+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.stages[-1].approxSimilarityJoin(result_imdb, result_lens, 0.5, \"jaccardDist\")\n",
    "print(f\"{result.count()} matches\")\n",
    "(result\n",
    " .select('datasetA.id', 'datasetA.Title', 'datasetB.Title', 'jaccardDist')\n",
    " .sort(F.col('datasetA.id'))\n",
    " .show(5, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Only keep single row with lowest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 matches\n",
      "+--------------------+--------------------+--------------------+\n",
      "|               Title|               Title|         jaccardDist|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|The Shawshank Red...|Shawshank Redempt...| 0.05555555555555558|\n",
      "|          Fight Club|          Fight Club|                 0.0|\n",
      "| Inglorious Basterds|Inglourious Basterds| 0.10526315789473684|\n",
      "|The Lord of the R...|Lord of the Rings...|0.045454545454545414|\n",
      "|        Forrest Gump|        Forrest Gump|                 0.0|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('datasetA.id')\n",
    "result = (result\n",
    "           .withColumn('minDist', F.min('jaccardDist').over(w))\n",
    "           .where(F.col('jaccardDist') == F.col('minDist'))\n",
    "           .drop('minDist'))\n",
    "print(f\"{result.count()} matches\")\n",
    "(result\n",
    " .select('datasetA.Title', 'datasetB.Title', 'jaccardDist')\n",
    " .sort(F.col('datasetA.id'))\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missed IDs: 27, 34, 49, 67, 69, 70, 89, 96\n",
    "* Faulty Matches: 2, 20, 29, 31, 62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+------------------+\n",
      "|               Title|               Title|Rating|            Rating|\n",
      "+--------------------+--------------------+------+------------------+\n",
      "|               Alien|               Alien|   8.5| 4.055518882196001|\n",
      "|Star Wars Episode...|Star Wars: Episod...|   8.7| 4.144122313069856|\n",
      "|       The Lion King|       The Lion King|   8.4|  3.14922480620155|\n",
      "|The Lord of the R...|Lord of the Rings...|   8.8| 4.091188818716808|\n",
      "|Once upon a Time ...|    Once Upon a Time|   8.6|3.3636363636363638|\n",
      "+--------------------+--------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('datasetA.Title', 'datasetB.Title', 'datasetA.Rating', 'datasetB.Rating').show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
